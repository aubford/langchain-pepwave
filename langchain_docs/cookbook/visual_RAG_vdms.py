#!/usr/bin/env python
# coding: utf-8

# # Visual RAG using VDMS
# Visual RAG is a framework that retrieves video based on provided user prompt. It uses both video scene description generated by open source vision models (ex. video-llama, video-llava etc.) as text embeddings and frames as image embeddings to perform vector similarity search using VDMS.

# ## Start VDMS Server
# Let's start a VDMS docker container using the port 55559.
# Keep note of the port and hostname as this is needed for the vector store as it uses the VDMS Python client to connect to the server.

# In[1]:


get_ipython().system(' docker run --rm -d -p 55559:55555 --name vdms_rag_nb intellabs/vdms:latest')


# ## Import Python Packages
# 
# Verify the necessary python packages are available for this visual RAG example.

# In[2]:


get_ipython().system(' pip install --quiet -U langchain-vdms langchain-experimental sentence-transformers opencv-python open_clip_torch torch accelerate')


# Now import the packages.

# In[3]:


import json
import os
from pathlib import Path
from threading import Thread
from typing import Any, List, Mapping, Optional
from zipfile import ZipFile

import cv2
import torch
from huggingface_hub import hf_hub_download
from IPython.display import Video
from langchain.llms.base import LLM
from langchain_community.embeddings.sentence_transformer import (
    SentenceTransformerEmbeddings,
)
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.runnables import ConfigurableField
from langchain_experimental.open_clip import OpenCLIPEmbeddings
from langchain_vdms.vectorstores import VDMS, VDMS_Client
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TextIteratorStreamer,
    set_seed,
)

set_seed(22)
number_of_frames_per_second = 2


# ## Initialize Vector Stores
# In this section, we initialize the VDMS vector store for both text and images. The text components use model `all-MiniLM-L12-v2`from `SentenceTransformerEmbeddings` and the images use model `ViT-g-14` from `OpenCLIPEmbeddings`.

# In[4]:


# Create directory to store data
datapath = Path("./data/visual").resolve()
datapath.mkdir(parents=True, exist_ok=True)

# Create directory to store frames
frame_dir = str(datapath / "frames_from_clips")
os.makedirs(frame_dir, exist_ok=True)

# Connect to VDMS server
vdms_client = VDMS_Client(port=55559)


# In[5]:


import warnings

warnings.filterwarnings("ignore")

# Initialize VDMS Vector Store
text_collection = "text-test"
text_embedder = SentenceTransformerEmbeddings(model_name="all-MiniLM-L12-v2")
text_db = VDMS(
    client=vdms_client,
    embedding=text_embedder,
    collection_name=text_collection,
    engine="FaissFlat",
)

text_retriever = text_db.as_retriever().configurable_fields(
    search_kwargs=ConfigurableField(
        id="k_text_docs",
        name="Search Kwargs",
        description="The search kwargs to use",
    )
)


# In[6]:


image_collection = "image-test"
image_embedder = OpenCLIPEmbeddings(
    model_name="ViT-g-14", checkpoint="laion2b_s34b_b88k"
)
image_db = VDMS(
    client=vdms_client,
    embedding=image_embedder,
    collection_name=image_collection,
    engine="FaissFlat",
)
image_retriever = image_db.as_retriever(search_type="mmr").configurable_fields(
    search_kwargs=ConfigurableField(
        id="k_image_docs",
        name="Search Kwargs",
        description="The search kwargs to use",
    )
)


# ## Data Loading
# 
# For this visual RAG example, we need to obtain videos and also video scene descriptions generated by open source vision models (ex. video-llava etc.) as text. 
# We have published a [Video Summarization Dataset](https://huggingface.co/datasets/Intel/Video_Summarization_For_Retail) available on Hugging Face which contains short videos of shoppers in a retail setting along with the corresponding textual description of each video.

# In[7]:


# Download data
hf_hub_download(
    repo_id="Intel/Video_Summarization_For_Retail",
    filename="VideoSumForRetailData.zip",
    repo_type="dataset",
    local_dir=str(datapath),
)
with ZipFile(str(datapath / "VideoSumForRetailData.zip"), "r") as z:
    z.extractall(path=datapath)

with open(str(datapath / "VideoSumForRetailData/clips_anno.json"), "r") as f:
    scene_info = json.load(f)

video_dir = str(datapath / "VideoSumForRetailData/clips/")

# Create dict for data where key is video name and value is scene description
video_list = {}
for scene in scene_info:
    video_list[scene["video"].split("/")[-1]] = scene["conversations"][1]["value"]


# Here we use OpenCV to extract metadata such as fps and number of frames for each video and also metadata such as frame number and timestamp for each extracted video frame. Once the metadata is extracted, the details are stored in VDMS.

# In[8]:


text_content = []
video_metadata_list = []
uris = []
frame_metadata_list = []
for video_name, description in video_list.items():
    video_path = os.path.join(video_dir, video_name)

    # Obtain Description and Video Metadata
    text_content.append(description)
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)
    metadata = {"video": video_name, "fps": fps, "total_frames": total_frames}
    video_metadata_list.append(metadata)

    # Obtain Metadata per Extracted Frame
    mod = int(fps // number_of_frames_per_second)
    if mod == 0:
        mod = 1
    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frame_count += 1
        if frame_count % mod == 0:
            timestamp = (
                cap.get(cv2.CAP_PROP_POS_MSEC) / 1000
            )  # Convert milliseconds to seconds
            frame_path = os.path.join(frame_dir, f"{video_name}_{frame_count}.jpg")
            cv2.imwrite(frame_path, frame)  # Save the frame as an image
            frame_metadata = {
                "timestamp": timestamp,
                "frame_path": frame_path,
                "video": video_name,
                "frame_num": frame_count,
            }
            uris.append(frame_path)
            frame_metadata_list.append(frame_metadata)
    cap.release()

# Add Text and Images
text_db.add_texts(text_content, video_metadata_list)
image_db.add_images(uris, frame_metadata_list);


# ## Run Multimodal Retrieval
# 
# Here we define helper functions for retrieving text and image results based on a user query.
# First, we use multi-modal retrieval to retrieve one text and three image documents for the user query. 
# Then we return the video name for the video with the most results.

# In[9]:


def MultiModalRetrieval(
    query: str,
    n_texts: Optional[int] = 1,
    n_images: Optional[int] = 3,
    print_text_content=False,
):
    text_config = {"configurable": {"k_text_docs": {"k": n_texts}}}
    image_config = {"configurable": {"k_image_docs": {"k": n_images}}}

    print("\tRetrieving 1 text doc and 3 image docs")
    text_results = text_retriever.invoke(query, config=text_config)
    image_results = image_retriever.invoke(query, config=image_config)

    if print_text_content:
        print(
            f"\tPage content:\n\t\t{text_results[0].page_content}\n\n\tMetadata:\n\t\t{text_results[0].metadata}"
        )

    return text_results + image_results


def get_top_doc(results, qcnt=0):
    hit_score = {}
    for r in results:
        if "video" in r.metadata:
            video_name = r.metadata["video"]
            if video_name not in hit_score.keys():
                hit_score[video_name] = 0
            hit_score[video_name] += 1

    x = dict(sorted(hit_score.items(), key=lambda item: -item[1]))

    if qcnt >= len(x):
        return None
    # print (f'top docs = {x}')
    return {"video": list(x)[qcnt]}


def Retrieve_top_results(prompt, qcnt=0, print_text_content=False):
    print("Querying database . . . ")
    results = MultiModalRetrieval(
        prompt, n_texts=1, n_images=3, print_text_content=print_text_content
    )
    print("Retrieved Top matching video!\n\n")

    top_doc = get_top_doc(results, qcnt)
    # print('TOP DOC = ', top_doc)
    if top_doc is None:
        return None, None

    return top_doc["video"], top_doc


# Now let's query for a `man wearing khaki pants` and retrieve the top results.

# In[10]:


input_query = "Find a man wearing khaki pants"
video_name, top_doc = Retrieve_top_results(input_query, print_text_content=True)


# ## Run RAG using LLM
# ### Load LLM Model
# In this example, we use Meta's [LLama-2-Chat (7B) model](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) which is optimized for dialogue use cases.  
# If you do not have access to this model, feel free to substitute the model with a different LLM.
# In this example, the model is expected to be in `data/visual/llama-2-7b-chat-hf`. 

# In[11]:


# Directory for LLM model
model_path = str(datapath / "llama-2-7b-chat-hf")

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float32,
    device_map="auto",
    trust_remote_code=True,
)

tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
tokenizer.padding_size = "right"
streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)


class CustomLLM(LLM):
    @torch.inference_mode()
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        streamer: Optional[TextIteratorStreamer] = None,  # Add streamer as an argument
    ) -> str:
        tokens = tokenizer.encode(prompt, return_tensors="pt")

        with torch.no_grad():
            output = model.generate(
                input_ids=tokens.to(model.device.type),
                max_new_tokens=100,
                num_return_sequences=1,
                num_beams=1,
                min_length=1,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.2,
                length_penalty=1,
                temperature=0.1,
                streamer=streamer,
                # pad_token_id=tokenizer.eos_token_id,
                do_sample=True,
            )

    def stream_res(self, prompt):
        thread = Thread(
            target=self._call, args=(prompt, None, None, streamer)
        )  # Pass streamer to _call
        thread.start()

        for text in streamer:
            yield text

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return model_path  # {"name_of_model": model_path}

    @property
    def _llm_type(self) -> str:
        return "custom"


llm = CustomLLM()


# ### Run Chatbot
# 
# First, we define the prompt and a simple chatbot for processing the user query.

# In[12]:


def get_formatted_prompt(scene, prompt):
    PROMPT = """ <<SYS>>
    You are an Intel assistant who understands visual and textual content.
    <</SYS>>
    [INST]
    You will be provided with two things, scene description and user's question. You are suppose to understand scene description \
    and provide answer to user's question.

    As an assistant, you need to follow these Rules while answering questions,

    Rules:
    - Don't answer any question which are not related to provided scene description.
    - Don't be toxic and don't include harmful information.
    - Answer if you can from provided scene description otherwise just say You don't have enough information to answer the question.

    Here is the,
    Scene Description: {{ scene }}

    The user wants to know,
    User: {{ prompt }}
    [/INST]\n
    Assistant:
    """
    return PROMPT.replace("{{ scene }}", scene).replace("{{ prompt }}", prompt)


def simple_chatbot(user_query):
    messages = [{"role": "assistant", "content": "How may I assist you today?"}]
    messages.append({"role": "user", "content": user_query})
    video_name, top_doc = Retrieve_top_results(user_query)

    scene_des = video_list[video_name]
    formatted_prompt = get_formatted_prompt(scene=scene_des, prompt=user_query)
    # print(formatted_prompt)
    full_response = f"Most relevant retrieved video is **{video_name}** \n\n"
    for new_text in llm.stream_res(formatted_prompt):
        full_response += new_text
    message = {"role": "assistant", "content": full_response}
    messages.append(message)

    for message in messages:
        print(message["role"].capitalize(), ": ", message["content"])

    video_path = os.path.join(video_dir, top_doc["video"])
    return video_path


# Now let's use the simple chatbot to process a query asking for a `man holding a red shopping basket` and display the resulting video.

# In[13]:


input_query = "Find a man holding a red shopping basket"
video_name, top_doc = Retrieve_top_results(input_query, print_text_content=True)


# In[14]:


input_query = "Find a man holding a red shopping basket"
video_path = simple_chatbot(input_query)


# In[15]:


Video(video_path, embed=True, width=640, height=360)


# ## Stop VDMS Server
# Now that we are done with the VDMS server, we can stop and remove it.

# In[16]:


get_ipython().system(' docker kill vdms_rag_nb')


# #
