#!/usr/bin/env python
# coding: utf-8
---
sidebar_label: Jina Search
---
# # Jina Search
# 
# This notebook provides a quick overview for getting started with Jina [tool](/docs/integrations/tools/). For detailed documentation of all Jina features and configurations head to the [API reference](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.jina_search.tool.JinaSearch.html).
# 
# ## Overview
# 
# ### Integration details
# 
# | Class | Package | Serializable | JS support |  Package latest |
# | :--- | :--- | :---: | :---: | :---: |
# | [JinaSearch](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.jina_search.tool.JinaSearch.html) | [langchain-community](https://python.langchain.com/api_reference/community/) | ❌ | ❌ |  ![PyPI - Version](https://img.shields.io/pypi/v/langchain-community?style=flat-square&label=%20) |
# 
# ### Tool features
# | [Returns artifact](/docs/how_to/tool_artifacts/) | Native async | Return data | Pricing |
# | :---: | :---: | :---: | :---: |
# | ❌ | ❌ | URL, Snippet, Title, Page Content | 1M response tokens free | 
# 
# 
# ## Setup
# 
# The integration lives in the `langchain-community` package and was added in version `0.2.16`:

# In[ ]:


get_ipython().run_line_magic('pip', 'install --quiet -U "langchain-community>=0.2.16"')


# ### Credentials

# In[2]:


import getpass
import os

if not os.environ.get("JINA_API_KEY"):
    os.environ["JINA_API_KEY"] = getpass.getpass("Jina API key:\n")


# It's also helpful (but not needed) to set up [LangSmith](https://smith.langchain.com/) for best-in-class observability:

# In[3]:


# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()


# ## Instantiation
# 
# - TODO: Fill in instantiation params
# 
# Here we show how to instantiate an instance of the Jina tool, with 

# In[2]:


from langchain_community.tools import JinaSearch

tool = JinaSearch()


# ## Invocation
# 
# ### [Invoke directly with args](/docs/concepts/tools)

# In[4]:


print(tool.invoke({"query": "what is langgraph"})[:1000])


# ### [Invoke with ToolCall](/docs/concepts/tools)
# 
# We can also invoke the tool with a model-generated ToolCall, in which case a ToolMessage will be returned:

# In[6]:


# This is usually generated by a model, but we'll create a tool call directly for demo purposes.
model_generated_tool_call = {
    "args": {"query": "what is langgraph"},
    "id": "1",
    "name": tool.name,
    "type": "tool_call",
}
tool_msg = tool.invoke(model_generated_tool_call)
print(tool_msg.content[:1000])


# ## Chaining
# 
# We can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:
# 
# import ChatModelTabs from "@theme/ChatModelTabs";
# 
# <ChatModelTabs customVarName="llm" />
# 

# In[ ]:


# | output: false
# | echo: false

# !pip install -qU langchain langchain-openai
from langchain.chat_models import init_chat_model

llm = init_chat_model(model="gpt-4o", model_provider="openai")


# In[10]:


from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig, chain

prompt = ChatPromptTemplate(
    [
        ("system", "You are a helpful assistant."),
        ("human", "{user_input}"),
        ("placeholder", "{messages}"),
    ]
)


llm_with_tools = llm.bind_tools([tool])
llm_chain = prompt | llm_with_tools


@chain
def tool_chain(user_input: str, config: RunnableConfig):
    input_ = {"user_input": user_input}
    ai_msg = llm_chain.invoke(input_, config=config)
    tool_msgs = tool.batch(ai_msg.tool_calls, config=config)
    return llm_chain.invoke({**input_, "messages": [ai_msg, *tool_msgs]}, config=config)


tool_chain.invoke("what's langgraph")


# ## API reference
# 
# For detailed documentation of all Jina features and configurations head to the API reference: https://python.langchain.com/api_reference/community/tools/langchain_community.tools.jina_search.tool.JinaSearch.html
