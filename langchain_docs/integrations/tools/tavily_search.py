#!/usr/bin/env python
# coding: utf-8

# # Tavily Search

# [Tavily's Search API](https://tavily.com) is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.
#
# ## Overview
#
# ### Integration details
# | Class | Package | Serializable | [JS support](https://js.langchain.com/docs/integrations/tools/tavily_search) |  Package latest |
# | :--- | :--- | :---: | :---: | :---: |
# | [TavilySearchResults](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html) | [langchain-community](https://python.langchain.com/api_reference/community/index.html) | ❌ | ✅ |  ![PyPI - Version](https://img.shields.io/pypi/v/langchain-community?style=flat-square&label=%20) |
#
# ### Tool features
# | [Returns artifact](/docs/how_to/tool_artifacts/) | Native async | Return data | Pricing |
# | :---: | :---: | :---: | :---: |
# | ✅ | ✅ | Title, URL, content, answer | 1,000 free searches / month |
#
#
# ## Setup
#
# The integration lives in the `langchain-community` package. We also need to install the `tavily-python` package.

# In[ ]:


get_ipython().run_line_magic(
    "pip", 'install -qU "langchain-community>=0.2.11" tavily-python'
)


# ### Credentials
#
# We also need to set our Tavily API key. You can get an API key by visiting [this site](https://app.tavily.com/sign-in) and creating an account.

# In[2]:


import getpass
import os

if not os.environ.get("TAVILY_API_KEY"):
    os.environ["TAVILY_API_KEY"] = getpass.getpass("Tavily API key:\n")


# It's also helpful (but not needed) to set up [LangSmith](https://smith.langchain.com/) for best-in-class observability:

# In[3]:


# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()


# ## Instantiation
#
# Here we show how to instantiate an instance of the Tavily search tools, with

# In[1]:


from langchain_community.tools import TavilySearchResults

tool = TavilySearchResults(
    max_results=5,
    search_depth="advanced",
    include_answer=True,
    include_raw_content=True,
    include_images=True,
    # include_domains=[...],
    # exclude_domains=[...],
    # name="...",            # overwrite default tool name
    # description="...",     # overwrite default tool description
    # args_schema=...,       # overwrite default args_schema: BaseModel
)


# ## Invocation
#
# ### [Invoke directly with args](/docs/concepts/tools)
#
# The `TavilySearchResults` tool takes a single "query" argument, which should be a natural language query:

# In[2]:


tool.invoke({"query": "What happened at the last wimbledon"})


# ### [Invoke with ToolCall](/docs/concepts/tools)
#
# We can also invoke the tool with a model-generated ToolCall, in which case a ToolMessage will be returned:

# In[5]:


# This is usually generated by a model, but we'll create a tool call directly for demo purposes.
model_generated_tool_call = {
    "args": {"query": "euro 2024 host nation"},
    "id": "1",
    "name": "tavily",
    "type": "tool_call",
}
tool_msg = tool.invoke(model_generated_tool_call)

# The content is a JSON string of results
print(tool_msg.content[:400])


# In[20]:


# The artifact is a dict with richer, raw results
{k: type(v) for k, v in tool_msg.artifact.items()}


# In[21]:


import json

# Abbreviate the results for demo purposes
print(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))


# ## Chaining
#
# We can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:
#
# import ChatModelTabs from "@theme/ChatModelTabs";
#
# <ChatModelTabs customVarName="llm" />
#

# In[ ]:


# | output: false
# | echo: false

# !pip install -qU langchain langchain-openai
from langchain.chat_models import init_chat_model

llm = init_chat_model(model="gpt-4o", model_provider="openai", temperature=0)


# In[23]:


import datetime

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig, chain

today = datetime.datetime.today().strftime("%D")
prompt = ChatPromptTemplate(
    [
        ("system", f"You are a helpful assistant. The date today is {today}."),
        ("human", "{user_input}"),
        ("placeholder", "{messages}"),
    ]
)

# specifying tool_choice will force the model to call this tool.
llm_with_tools = llm.bind_tools([tool])

llm_chain = prompt | llm_with_tools


@chain
def tool_chain(user_input: str, config: RunnableConfig):
    input_ = {"user_input": user_input}
    ai_msg = llm_chain.invoke(input_, config=config)
    tool_msgs = tool.batch(ai_msg.tool_calls, config=config)
    return llm_chain.invoke({**input_, "messages": [ai_msg, *tool_msgs]}, config=config)


tool_chain.invoke("who won the last womens singles wimbledon")


# Here's the [LangSmith trace](https://smith.langchain.com/public/b43232c1-b243-4a7f-afeb-5fba8c84ba56/r) for this run.

# ## API reference
#
# For detailed documentation of all TavilySearchResults features and configurations head to the API reference: https://python.langchain.com/api_reference/community/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html
